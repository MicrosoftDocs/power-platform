---
title: Distributed tracing across multiple services​ 
description: Learn how to trace events across multiple Power Platform services.
#customer intent: As a flowmaker, I want to trace events that occur across multiple services in my Power Platform solution.
author: manuelap-msft
ms.subservice: architecture-center
ms.topic: example-scenario
ms.date: 04/22/2025
ms.author: mapichle
ms.reviewer: pankajsharma2087
contributors:
  - manuelap-msft
ms.contributors:
  - grarchib
search.audienceType:
  - admin
  - flowmaker
---

# ​​Distributed tracing across multiple services​ 

This article explains how to achieve comprehensive observability across multiple services, including those within Power Platform, Azure, and Dynamics 365.

> [!TIP]
> The article provides an example scenario and visual representation of how to trace events across multiple services. This solution is a generalized example scenario architecture, which can be used for many different scenarios and industries.

Monitoring tools and processes are essential for maintaining the health and performance of applications and services. They help in tracking metrics, logs, and traces to identify issues and optimize performance. [Azure Monitor](/azure/azure-monitor/fundamentals/overview) is a comprehensive solution for collecting, analyzing, and acting on telemetry from your cloud and on-premises environments. It uses [Kusto Query Language (KQL)](/kusto/query/) to query and correlate telemetry data across different components. Monitoring tools and processes are a well understood pattern. [Create KQL queries to query and correlate telemetry across different Components](/azure/aks/monitor-aks?tabs=cilium) and [Query data in Azure Monitor with Azure Data Explorer](/azure/data-explorer/query-monitor-data) demonstrate how to add and query generated monitoring data. This pattern builds upon existing methods to achieve comprehensive observability across multiple services, including those within Power Platform, Azure, and Dynamics 365.

## Architecture diagram

:::image type="content" source="media/distributed-tracing/distributed-tracing.png" alt-text="Diagram of distributed tracing across multiple Power Platform services​ ." lightbox="media/distributed-tracing/distributed-tracing.png":::

## Workflow

### Azure to Dataverse Web API workflow

1. **​​End user applications**: Azure Services and Solutions like Azure Functions, Web Services and Kubernetes can start a Distributed transaction relating to a specific event generated by the user or an agent.
1. **​Dataverse Web API**: Azure Services can add a W3C traceparent to requests to Dataverse Entities and Custom APIs. This traceparent can be included in the body of the request or in the tag query string of the request.
1. **​Dataverse messages**: Requests sent to Dataverse are sent as Messages. These messages could relate to an Entity or a custom defined API. Messages can have Pre and Post actions applied to them. These actions could be a Application Insights distributed tracking 
1. **​Plugins**: Make use of Dataverse C# Plugin to generate Distributed Tracing telemetry to relate the Azure Request to Dataverse action. 
1. **​Azure Monitor**: Create KQL queries to query and correlate telemetry across different Components.

### ​Power Platform Workflow 

1. **​Instrument**: Configure the Power Platform resource with the Application Insights Connection String / Key 
1. **Trace**: Microsoft Copilot Studio, Power Apps, Power Automate can begin a transaction by calling a Custom Dataverse API.​ 

## Components

- **[​Application Insights](/azure/azure-monitor/app/app-insights-overview)**: Application Insights is a feature of [Azure Monitor](/azure/azure-monitor/overview), an extensible Application Performance Management (APM) tool that allows you to monitor your live applications. It requires a subscription to [Microsoft Azure](https://azure.microsoft.com/). Application Insights is used for storing trace and dependency information, and enables searching for operation IDs.
- **​​Azure services**: [Monitor Azure resources with Azure Monitor](/azure/azure-monitor/app/app-insights-overview). While resources from different Azure services have different monitoring requirements, they generate monitoring data in the same formats so that you can use the same Azure Monitor tools to analyze all Azure resources.
- **[​Copilot Studio](/microsoft-copilot-studio/fundamentals-what-is-copilot-studio)**: You can [capture telemetry data from your Copilot Studio agent](/microsoft-copilot-studio/advanced-bot-framework-composer-capture-telemetry?tabs=webApp) for use in Application Insights. To connect your agent to Application Insights, you first need to [add your instrumentation key to your agent's configuration](/microsoft-copilot-studio/advanced-bot-framework-composer-capture-telemetry?tabs=webApp#connect-your-copilot-studio-agent-to-application-insights). You can call an Environment Custom API from your agent to start a distributed transaction.
- **[​Power Apps canvas apps](/power-apps/maker/canvas-apps/)**: You can [connect your canvas apps to Application Insights](/power-apps/maker/canvas-apps/application-insights). You can call an Environment Custom API from your canvas app to start a distributed transaction.
- **[​Power Apps model-driven apps](/power-apps/maker/model-driven-apps/)**: You can [set up an Application Insights environment](/power-platform/admin/analyze-telemetry) to receive telemetry on diagnostics and performance captured by the Dataverse platform. You can subscribe to receive telemetry about operations that applications perform on your Dataverse database and within model-driven apps. You can call an Environment Custom API to start a distributed transaction.
- **[​Power Automate](/power-automate/)**: You can [enable export of cloud flow telemetry data](/power-platform/admin/app-insights-cloud-flow) to Application Insights at the environment. You can pass a distributed trace ID from Microsoft Copilot Studio or Power Apps to a cloud flow.
- **[Microsoft ​Dataverse](/power-apps/maker/data-platform/data-platform-intro)**: You can [set up an Application Insights environment](/power-platform/admin/analyze-telemetry) to receive telemetry on diagnostics and performance captured by the Dataverse platform.
- **[​Dataverse plug-ins](/power-apps/developer/data-platform/plug-ins)**: You can use C# Dataverse Plugins to provide dependency records and integrate with Application Insights.
- **[​Azure Data Explorer](/azure/data-explorer/data-explorer-overview)**: You can use Azure Data Explorer to use KQL Queries join data across multiple sources.
- **[Power BI](/power-bi/)**: You can visualize and query telemetry data​ with Power BI.

## Scenario details

​​This architecture is designed to allow telemetry to be correlated across multiple components within the Power Platform and from systems outside the Power Platform by implementing the [W3C Distributed Tracing](https://www.w3.org/TR/trace-context/) standard so that:

- ​Allow Trace and Request records in Application Insights to be correlated using dependency records.
- Allow end to end traceability of operations.
- ​Allow support teams to configure telemetry to diagnose and resolve issues.
- ​Demonstrate how external systems can pass distributed tracing context to the Power Platform.
- ​Demonstrate how Power Platform components can be part of a distributed tracing session.​

### Distributed Tracing

Distributed tracing is a method used to profile and monitor applications, especially those built using a microservices architecture. It allows you to trace an event in the system from one service to another and retrieve end-to-end diagnostics about performance and latency. The [W3C TraceContext](https://www.w3.org/TR/trace-context/) standard defines how context information is sent and modified between services, enabling distributed tracing scenarios.

#### Trace ID and Span ID

In the W3C TraceContext standard, each trace is assigned a globally unique 16-byte trace-id, and every activity within the trace is assigned a unique 8-byte span-id. The trace-id represents the overall transaction, while the span-id represents individual operations within that transaction. Each activity records the trace-id, its own span-id, and the span-id of its parent, establishing parent-child relationships between activities.

#### Example Scenario

Let's consider an example where a browser starts a transaction, multiple microservices interact, and a call is made to the Dataverse WebApi

| Source | Trace ID and Span ID |
| --- | --- |
| Browser | traceparent: 00-11111111111111111111111111111111-2222222222222222-01 |
| Kubernetes | traceparent: 00-11111111111111111111111111111111-3333333333333333-01 |
| Dataverse | traceparent: 00-11111111111111111111111111111111-4444444444444444-01 |

1. **Browser initiates transaction**: The browser sends a request to a web server. This request is assigned a trace-id and a span-id (let's call it span-id-1).
1. **Microservices interaction**: The web server processes the request and makes a call to a microservice. This call is assigned a new span-id (span-id-2) but retains the same trace-id. The microservice, in turn, calls another microservice, creating another span-id (span-id-3), and so on.
1. **Call to Dataverse Web API**: One of the microservices makes a call to the Dataverse Web API. This call is assigned a new span-id (span-id-4) but retains the same trace-id.

#### Trace ID and parent-child relationship

The trace-id is used to join all transactions with a parent-child relationship. Each span-id represents a unique operation within the trace, and the parent span-id links it to its parent operation. This hierarchical structure allows you to trace the entire transaction from the initial request to the final response, even as it traverses multiple services and systems.

#### Trace ID and Operation ID lookup

The W3C TraceId field is mapped in Application Insights to the Operation Id. This allows you to easily query a set of related actions that occur as part of the end to end trace.

:::image type="content" source="media/distributed-tracing/trace-id-field.png" alt-text="Screenshot of the Operation Id field mapped in Application Insights​ ." lightbox="media/distributed-tracing/trace-id-field.png":::

#### Practical Applications and Benefits

Using the W3C TraceContext standard for distributed tracing offers several practical applications and benefits:

1. End-to-End Visibility: It provides end-to-end visibility into the transaction, helping you understand how requests propagate through your system.
1. Performance Monitoring: It allows you to monitor the performance of individual services and identify bottlenecks.
1. Error Diagnosis: It helps in diagnosing errors by tracing the path of a request and identifying where failures occur.
1. Dependency Tracking: It enables you to track dependencies between services and understand how they interact.

By implementing distributed tracing with the W3C TraceContext standard, you can gain valuable insights into your application's behaviour, improve performance, and enhance the overall user experience.



### Potential use cases

| Example | Description | Notes |
| --- | --- |--- |
| Autonomous agent | An autonomous agent is triggered by a data event. The possible longed live transaction persists the traceparent. This transaction could cross multiple processes and services including possible handoff to a customer service agent. | Power Automate can request distributed tracing from Dataverse Plugin. Each step of the process added Application Insights telemetry. E2E transaction could be queries in Application Insights or via KQL queries. |
| End User Web, Mobile or Agent Transaction | A user starts a transaction to update their customer data. Trace entries added to Application insights from the Request and Trace messages from Dataverse | Kubernetes service starts a distribute transaction. It calls the Dataverse WebApi to update customer details. |
| Customer Support Agent | End customer contacts the call centre. A call centre operator make use of Copilot and Power App to update the customer details. Each component in the update writes to Application Insights | Transaction starts with call centre operator. The Power App Request distributed transaction from Dataverse. |

## Considerations

These considerations implement the pillars of Power Platform Well-Architected, a set of guiding tenets that improve the quality of a workload. Learn more in [Microsoft Power Platform Well-Architected](/power-platform/well-architected/).

### Reliability

Ensure that the implemented solution fit into your [monitoring and alerting strategy](/power-platform/well-architected/performance-efficiency/collect-performance-data).

### Operational excellence

To effectively monitor your workload for security, performance, and reliability, you need a comprehensive system with its own stack that provides the foundation for all monitoring, detection, and alerting functions. Learn more: [Recommendations for designing and creating a monitoring system](https://learn.microsoft.com/en-us/power-platform/well-architected/operational-excellence/observability)

#### Performance efficiency

The suggestions in this example scenario allow you to [collect workload performance data recommendation for Power Platform workloads](/power-platform/well-architected/performance-efficiency/collect-performance-data)

## Contributors

_Microsoft maintains this article. The following contributors wrote this article._

Principal authors:

- **[Grant Archibald](https://www.linkedin.com/in/grantarchibald/)**, Senior Program Manager

## Next steps

>[!TIP]
> The [Dataverse Open Telemetry sample](https://github.com/Grant-Archibald-MS/dataverse-opentelemetry) project contains an example of Dataverse plugin integration with Open Telemetry. OpenTelemetry is a collection of W3C Standads, APIs, SDKs, and tools. Use it to instrument, generate, collect, and export telemetry data (metrics, logs, and traces) to help you analyze your software’s performance and behavior.

### Dataverse Web API integration

Let's explore how the [Dataverse Web API](/power-apps/developer/data-platform/webapi/overview) can be integrated with W3C TraceContext to enable distributed tracing.

The calling service initiates a trace with a unique trace-id and span-id. This traceparent value can be passed to the Web API either in the body of an HTTP POST request or as part of an HTTP query string.

- **Option 1: POST body**: `postData(environmentUrl + "api/data/v9.0/" + customApiName, token, ...)`
- **Option 2: Tag query string**: `postData(environmentUrl + "api/data/v9.0/" + customApiName + "?tag=01-0af...")`

By using either method, a Dataverse plug-in can be configured to incorporate Application Insights tracing, generating new span-ids and trace messages.

### Dataverse integration

Lets explore how this pattern can implemented making use of the Generally Available features of dataverse.

To apply distributed tracing to calls to the [Dataverse WebApi](/power-apps/developer/data-platform/webapi/overview) two approachs will be combined Dataverse Messages to extend the message pipeline and a Custom API using Datavesrse Plugins that make use of the Application Insights SDK to add the required parent child relationships.

#### Invoke from other Power Platform Services

When looking at other components in the Power Platform you could also consider making use of [Invoke a function from app, flow, code, or another function](/power-apps/maker/data-platform/functions-invoke) to invoke the Dataverse Custom APIs discussed in this section.

This approach allows serviecs like Microsoft Copilot Studio, Power Apps or Power Automate Cloud Flows to be included in the overall distributed tracing solution. 

#### Dataverse messages for entities or custom APIs

Custom [Dataverse messages](/power-apps/developer/data-platform/custom-actions) can be defined that allow you to interact with entities or custom APIs within the Dataverse environment. These messages enable you to perform operations such as create, update, delete, and retrieve data. By using Dataverse messages, you can streamline your data management processes and ensure seamless integration with your observability needs.

#### Adding steps to a plug-in

Now that a dataverse message type is created or using any of the predefined entity types of an environment you can take advantage of [Dataverse plug-ins](/power-apps/developer/data-platform/plug-ins) so that they can be configured to be executed at different stages of the data processing pipeline and execute distributed tracing.

These stages can include pre-validation, pre-operation, and post-operation steps. By adding steps to a plugin, you can control the flow of data and ensure that specific actions are taken at the right time.

- **Pre-validation**: This step occurs before the main operation is executed. It is used to validate the data and ensure that it meets the required criteria.
- **Pre-operation**: This step occurs after the pre-validation step but before the main operation is executed. It is used to perform any necessary preparations or modifications to the data.
- **Post-operation**: This step occurs after the main operation is executed. It is used to perform any necessary cleanup or additional actions based on the results of the main operation.

Plugins can be configured to execute these steps either synchronously or asynchronously, depending on the requirements of your application.

#### Plugin Configuration - Unsecure and secure configuration values

Specifically looking at scenarios for distributed tracing you can can use the unsecure configuration to manage if tracing should be enabled and the level of loggring that is applied. You could use the secure configuration value to store connection string information required by the plugin.

The process is managed when [registering plug-ins](/power-apps/developer/data-platform/register-plug-in), you can have both unsecure and secure [configuration values](/power-apps/developer/data-platform/register-plug-in#set-configuration-data). These values are used to control various aspects of the plugin's behavior.

- **Unsecure Configuration**: Unsecure settings are visible to all users and can include settings such as log level, enable/disable tracing, and other non-sensitive information.
- **Secure Configuration**: Secure settings are only visible to users with the appropriate permissions and can include sensitive information such as connection strings, API keys, and other confidential data.

:::image type="content" source="media/distributed-tracing/secure-config-values.png" alt-text="Screenshot of registering a plug-in." :::

By using secure configuration values, you can ensure that sensitive information is protected and only accessible to authorized users.

#### Custom API with request and response parameters

Dataverse allows you to define [custom APIs](/power-apps/developer/data-platform/custom-api) with specific request and response parameters. This feature enables you to create tailored APIs that meet the unique needs of your application.

- [**Input parameters**](/power-apps/developer/data-platform/understand-the-data-context#inputparameters): These parameters define the input data required by the custom API. They can include various data types such as strings, integers, and complex objects.
- [**Output parameters**](/power-apps/developer/data-platform/understand-the-data-context#outputparameters): These parameters define the output data returned by the custom API. They can include various data types and structures, allowing you to provide detailed and meaningful responses to API consumers.

:::image type="content" source="media/distributed-tracing/custom-api.png" alt-text="Screenshot of registering a custom API" :::

> [!TIP]
> In the case of distributed tracing you could tag query string value for [Passing a Shared Variable from the API](/power-apps/developer/data-platform/understand-the-data-context#passing-a-shared-variable-from-the-api)

#### Custom processing steps (sync and async)

When using custom processing steps, you can define whether the steps should be executed synchronously or asynchronously. This flexibility allows you to optimize the performance and responsiveness of your application.

- **Synchronous processing**: In synchronous processing, the steps are executed in a sequential manner, and the next step is not initiated until the current step is completed. This approach ensures that each step is completed before moving on to the next one.
- **Asynchronous processing**: In asynchronous processing, the steps are executed independently, and the next step can be initiated before the current step is completed. This approach allows for parallel processing and can improve the overall performance of your application.

By defining custom processing steps, you can add monitoring and other functionalities to existing entities or custom API messages, ensuring that your application operates efficiently and effectively.

### Dataverse C# Plugin

Dataverse has the ability to [write plug-ins](/power-apps/developer/data-platform/write-plug-in). This feature can be used to build and deploy a C# plugins that make use of the Application Insights SDK to create the correct Parent child relationship between services.

### Comparison with out-of-the-box ILogger

Dataverse provides an [out-of-the-box ILogger](/power-apps/developer/data-platform/application-insights-ilogger) that can be configured at the environment level. This ILogger is designed to offer a standardized logging mechanism across different environments, ensuring consistency and ease of use. However, it may not provide the same level of granularity and customization as the custom plugin based ILogger.

#### Custom plug-in ILogger

A custom plugin ILogger in Dataverse offers more detailed levels of information, such as Trace, Debug, and Information. This allows developers to capture more specific and relevant data during the execution of plugins. The Plugin ILogger can utilize values from the message request or the Share Variable tag parameter to specify the calling traceparent, enabling better tracking and correlation of logs.

#### Key C# concepts for parsing activity and specifying Parent ID

When working with the custom plugin ILogger, it's essential to understand key C# concepts for parsing activity and specifying the parent ID. Here is an example of how to create a new activity for a trace message:

```csharp
// Create a new activity for the trace message
var activity = new Activity("CustomActivity");
activity.SetParentId(traceParent);
activity.Start();
// Create a trace telemetry record
var traceTelemetry = new TraceTelemetry(message, ConvertLogLevel(level))
{
    Message = message,
    Context = { Operation = { ParentId = dependencyTelemetry.Id, Id = activity.Id } }
};
// Track the trace telemetry
telemetryClient.TrackTrace(traceTelemetry);
```

## Related resources

- [W3C ​​Trace Context standard defining traceparent and tracestate http headers](https://www.w3.org/TR/trace-context/)
- [​Event Framework (Microsoft Dataverse)](/power-apps/developer/data-platform/event-framework)
- [​Understand the execution context (Microsoft Dataverse)](/power-apps/developer/data-platform/understand-the-data-context#passing-a-shared-variable-from-the-api)
